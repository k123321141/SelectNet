## Abstract

> Feature selection plays an important role in many machine learning and data mining applications. We propose a regularization loss on feature space, <a href="https://www.codecogs.com/eqnedit.php?latex=x,&space;w&space;\in&space;R^d,&space;\hat&space;y=&space;f(x\cdot&space;w|\theta)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?x,&space;w&space;\in&space;R^d,&space;\hat&space;y=&space;f(x\cdot&space;w|\theta)" title="x, w \in R^d, \hat y= f(x\cdot w|\theta)" /></a> ,to force the model using the less feature group to solve the original problem. As common regularization loss in Deep Learning, we apply L1 norm on w, and use SGD to make it converged.
>
# Introduction
Deep learning ä¸€èˆ¬ä¾†èªªæ˜¯ä¸åšfeature selectionï¼Œå…¶å„ªç•°çš„åˆ†é¡èƒ½åŠ›ï¼Œä½¿äººå€‘ä»ç„¶èƒ½å¤ æ¥å—é»‘ç®±çš„ç¼ºé»ã€‚</br>
Feature selection åœ¨Networkä¸Šæœ‰å¹¾ç¨®åšæ³•ï¼š</br>
1. weight based : é‡å°èˆ‡æŸé …featureç›¸é—œçš„weightï¼Œè¶Šé‡è¦çš„featureå…¶è¡¨ç¾æœƒä½œç”¨åœ¨é€™äº›ç›¸é—œçš„weightä¸Šã€‚
2. output sensitivity based : é‡å°æŸé …ç‰¹å®šçš„featureå°æ–¼æ¨¡å‹çš„è¼¸å‡ºå½±éŸ¿åŠ›å¤§å°ï¼Œå¸¸è¦‹åšæ³•æ˜¯åšfeature rankingï¼Œç„¶å¾Œé€æ¬¡ç§»é™¤featureã€‚


weight basedçš„ç¼ºé»æ˜¯ï¼Œnetworkçš„è¤‡é›œåº¦è¶Šå¤§ï¼Œè¶Šé›£ä»¥åˆ†æweightä¹‹é–“çš„ä½œç”¨ç¨‹åº¦ï¼ŒåŒ…æ‹¬shared-weightä»¥åŠactivation functionã€‚</br>
è€Œoutput sensitivity basedçš„ç¼ºé»æœ‰å…©å€‹ï¼Œé™¤å»ç‰¹å®šfeatureçš„éç¨‹å±¬æ–¼greedy strategyï¼Œä¸¦ä¸”é‡è¤‡è¨“ç·´çš„éç¨‹éå¸¸è€—è²»é‹ç®—è³‡æºã€‚</br>
é€™ç¯‡çš„åšæ³•é€éè§€å¯Ÿwæ”¶æ–‚çš„è¶¨å‹¢ï¼Œäº†è§£ä½œç”¨åœ¨feature spaceä¸Šçš„æ©Ÿç‡åˆ†ä½ˆã€‚


# Original regression loss in Deep learning

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}&space;=&space;f&space;\left&space;(&space;x&space;\mid&space;\Theta&space;\right&space;),&space;x\in&space;R^d&space;\newline&space;Loss&space;=\alpha\left&space;\|&space;\Theta&space;\right&space;\|_2&space;&plus;(\hat{y}&space;-&space;y)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}&space;=&space;f&space;\left&space;(&space;x&space;\mid&space;\Theta&space;\right&space;),&space;x\in&space;R^d&space;\newline&space;Loss&space;=\alpha\left&space;\|&space;\Theta&space;\right&space;\|_2&space;&plus;(\hat{y}&space;-&space;y)^2" title="\hat{y} = f \left ( x \mid \Theta \right ), x\in R^d \newline Loss =\alpha\left \| \Theta \right \|_2 +(\hat{y} - y)^2" /></a></br></br>

å·¦é‚Šæ˜¯L2 regularization ternï¼Œåˆæ­¥çš„æƒ³æ³•æ˜¯å†å¢åŠ ä¸€å€‹é™åˆ¶é …ã€‚</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{y}&space;=&space;f&space;\left&space;(&space;w&space;\odot&space;x&space;\mid&space;\Theta&space;\right&space;),&space;x,&space;w\in&space;R^d&space;\newline&space;Loss&space;=\alpha\left&space;\|&space;\Theta&space;\right&space;\|_2&space;&plus;&space;\beta\left&space;\|&space;w&space;\right&space;\|_1&plus;&space;(\hat{y}&space;-&space;y)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{y}&space;=&space;f&space;\left&space;(&space;w&space;\odot&space;x&space;\mid&space;\Theta&space;\right&space;),&space;x,&space;w\in&space;R^d&space;\newline&space;Loss&space;=\alpha\left&space;\|&space;\Theta&space;\right&space;\|_2&space;&plus;&space;\beta\left&space;\|&space;w&space;\right&space;\|_1&plus;&space;(\hat{y}&space;-&space;y)^2" title="\hat{y} = f \left ( w \odot x \mid \Theta \right ), x, w\in R^d \newline Loss =\alpha\left \| \Theta \right \|_2 + \beta\left \| w \right \|_1+ (\hat{y} - y)^2" /></a></br></br>


é€éå¢åŠ L1 lossä½¿å¾—wæ”¶æ–‚æˆæ¯”è¼ƒsparseçš„å‘é‡ï¼Œæœ‰äº›å€¼ç‚ºé›¶ã€‚</br>
é€™æ™‚å€™å¯ä»¥å¾—çŸ¥å“ªäº›featureï¼Œæ˜¯ä¸éœ€è¦ä½¿ç”¨çš„ã€‚</br>
è©³ç´°çš„è¨ˆç®—å¼å®šç¾©ï¼š</br>

<a href="https://www.codecogs.com/eqnedit.php?latex=w'&space;=&space;sigmoid(w)&space;\newline&space;w_{ratio}&space;=&space;w'&space;/&space;\sum^d_i&space;w_i',&space;w_{ratio}&space;\in&space;R^d&space;\newline&space;\hat{y}&space;=&space;f&space;\left&space;(&space;w_{ratio}&space;\odot&space;x&space;\mid&space;\Theta&space;\right&space;),&space;x,&space;w\in&space;R^d&space;\newline&space;Loss&space;=\alpha\left&space;\|&space;\Theta&space;\right&space;\|_2&space;&plus;&space;\beta\left&space;\|&space;w'&space;\right&space;\|_1&plus;&space;\gamma&space;entropy\left&space;(&space;w_{ratio}&space;\right&space;)&space;&plus;(\hat{y}&space;-&space;y)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?w'&space;=&space;sigmoid(w)&space;\newline&space;w_{ratio}&space;=&space;w'&space;/&space;\sum^d_i&space;w_i',&space;w_{ratio}&space;\in&space;R^d&space;\newline&space;\hat{y}&space;=&space;f&space;\left&space;(&space;w_{ratio}&space;\odot&space;x&space;\mid&space;\Theta&space;\right&space;),&space;x,&space;w\in&space;R^d&space;\newline&space;Loss&space;=\alpha\left&space;\|&space;\Theta&space;\right&space;\|_2&space;&plus;&space;\beta\left&space;\|&space;w'&space;\right&space;\|_1&plus;&space;\gamma&space;entropy\left&space;(&space;w_{ratio}&space;\right&space;)&space;&plus;(\hat{y}&space;-&space;y)^2" title="w' = sigmoid(w) \newline w_{ratio} = w' / \sum^d_i w_i', w_{ratio} \in R^d \newline \hat{y} = f \left ( w_{ratio} \odot x \mid \Theta \right ), x, w\in R^d \newline Loss =\alpha\left \| \Theta \right \|_2 + \beta\left \| w' \right \|_1+ \gamma entropy\left ( w_{ratio} \right ) +(\hat{y} - y)^2" /></a></br></br>

ç›®å‰æ­£åœ¨ç ”ç©¶åŠ å…¥entropyæ˜¯å¦æœ‰å¹«åŠ©ã€‚</br>
é€éå°sigmoid(w)åšL1 lossè€Œä¸æ˜¯ç›´æ¥é‡å°wï¼Œä½¿å¾—w'å¯ä»¥æŒçºŒä¸‹é™åˆ°è² æ•¸ï¼Œç›¸è¼ƒæ–¼wåªèƒ½ä¸‹é™åˆ°0æ›´åŠ smooth(?)ã€‚</br>
(CKè€å¸«å»ºè­°ç›´æ¥ä½¿ç”¨reluä¾†è©¦è©¦çœ‹ã€‚)</br>
å¦å¤–å¯ä»¥ä½¿ç”¨softmaxæˆ–æ˜¯summation ratioä¾†åˆ©ç”¨w_prineå½¢æˆfeature saliencyã€‚</br>
ç›®å‰æ¡ç”¨summation ratioï¼ŒåŸå› ä¹Ÿæ˜¯å› ç‚ºsummationæ¯”è¼ƒä¸é™¡å³­ã€‚(éœ€è¦è£œå……èªªæ˜)</br>
(åœ¨w'æ¥µå°å€¼æ™‚ï¼Œlossæœƒæ¯”softmaxæŒ‡æ•¸é‚„è¦å¤§ï¼Œè—‰æ­¤æ¯”è¼ƒå®¹æ˜“ä¸‹é™åˆ°0ã€‚)</br>

ç”±æ­¤ä¸€ä¾†é€éè§€å¯Ÿw_ratioå°±å¯ä»¥ç†è§£featureå¦‚ä½•ç¶“éw_ratioçš„weight maskç„¶å¾Œé€²å…¥æ¨¡å‹çš„è¼¸å…¥ã€‚</br>
å¯ä»¥è¨­å®šæŸå€‹thresholdä¾†åšfeature selectionï¼Œå¾Œé¢MNISTçš„ä¾‹å­å°±æ˜¯å¦‚æ­¤ã€‚

ç›®å‰åªæ˜¯å¯¦é©—çŒœæ¸¬ï¼Œä¸‹åˆ—å˜—è©¦éçš„[Loss function](#other-loss)




## Toy example, bmi
y = w / (h^2) + small noise, scalar</br>
Xæ˜¯å››ç¶­çš„å‘é‡ï¼Œå‰å…©ç¶­x1,x2 = w, hï¼Œå¾Œå…©ç¶­x3,x4åˆ†åˆ¥æ˜¯ä¸é‡è¦çš„normal distrbutionã€‚</br></br>
mse lossç›¸ç•¶ä½ä»£è¡¨networkå¯ä»¥fit é€™å€‹å›æ­¸å•é¡Œã€‚</br>
wä¹ŸæŒçºŒä¸‹é™ä¸­ï¼Œè€Œæœ‰å¹¾é …ä¸ç›¸é—œçš„é …ï¼Œä¾‹å¦‚w3, w4ä¸‹é™çš„æ¯”è¼ƒå¿«</br></br>
![bmi w chart][bmi_w]</br></br>
æœ€çµ‚ratio_wé¡¯ç¤ºï¼ŒNetworkåªä½¿ç”¨äº†å‰å…©é …w,hå°±å¯ä»¥fité€™å€‹å›æ­¸å•é¡Œï¼Œè¡¨ç¤ºé€™å€‹regularization ternå¯ä»¥æ‰¾å‡ºå“ªäº›featureæ‰æ˜¯networkæœ‰ä½¿ç”¨åˆ°çš„ã€‚</br>
w3, w4è¶¨è¿‘æ–¼0</br></br>
![bmi ratio_w chart][bmi_w_ratio]</br></br>
summary</br></br>
![bmi log summary][bmi_summary]</br>

## MNIST example
ç”±æ–¼numerical dataæ¯”è¼ƒé›£ä»¥æ‰¾å‡ºfeature saliencyã€‚</br>
é€™é‚Šæœ‰åšç¸®æ”¾åˆ°10x10</br>
å¯¦é©—è¨­å®šçš„thresholdæ˜¯0.001</br>
çœ‹çœ‹w_ratioï¼Œçœ‹å¾—å‡ºæ˜é¡¯å·®ç•°ï¼Œæˆ‘è§£é‡‹æˆæŸäº›pixelæ˜¯æ¯”è¼ƒé‡è¦çš„ã€‚</br>
å†æ›´é€²ä¸€æ­¥åœ°å»é™¤w_ratio < thresholdçš„pixelï¼Œå…©æ¬¡åŒåƒæ•¸çš„è¨“ç·´çµæœå¦‚ä¸‹ã€‚</br>
![mnist_mask_1][mnist_mask_1]![mnist_mask_2][mnist_mask_2]</br>
è¨“ç·´éç¨‹:</br>
![mnist_overview_1][mnist_overview_1]![mnist_overview_2][mnist_overview_2]</br>


## CIFAR-10 example
ç›¸è¼ƒæ–¼mnistæ‰¾å‡ºäº†æœ‰äº›pixelå¯ä»¥ä¸ä½¿ç”¨ï¼Œcifar-10æ‰¾ä¸å‡ºä¾†æ˜é¡¯ä¸é‡è¦çš„pixelã€‚</br>
ä»¥ä¸‹æ˜¯cifar-10çš„ç¯„ä¾‹åœ–ï¼Œæˆ‘èªçˆ²é€™å€‹çµæœæ˜¯åˆç†çš„ã€‚</br>
![cifar10 example][cifar10_1]</br>

### å°æ–¼éç›¸é—œé …çš„å™ªéŸ³æŠµæŠ—åŠ›



### å–®è®Šé‡çµ±è¨ˆ


## other loss
Loss

### èˆ‡å–®è®Šé‡çµ±è¨ˆä»¥åŠç·šæ€§æ¨¡å‹çš„æ¯”è¼ƒï¼ŒXORè³‡æ–™

### feature ranking
æœ‰æ²’æœ‰å¯èƒ½æ¯æ¬¡ç”¨ä¸åŒçµ„çš„featureå»åšéæ¿¾ï¼Œç„¶å¾Œç•™ä¸‹ä¾†çš„featureå°±å¾—åˆ†ã€‚</br>
ä»¥å¾—åˆ†é«˜ä½ä¾†èªªæ˜featureçš„é‡è¦æ€§ã€‚</br>

### related papers
1. A Penalty-Function Approach for Pruning Feedforward Neural Networks, IEEE, 1997. 246 citations. </br></br>
short critique : ä½¿ç”¨input-hidden-outputå…±ä¸‰å±¤çš„FCNä½œç‚ºç¯„ä¾‹ï¼Œå°output nodeæœ‰å½±éŸ¿çš„å…±æœ‰å…©å±¤learnable matrix, w1,w2</br>
åªè¦å…¶ä¸­æŸä¸€æ¢link(tanh(x*w1_i)*w2_j)çš„å€¼è¶³å¤ å°ä¾¿è¡¨ç¤ºé€™æ¢linkå¯ä»¥åˆªé™¤ï¼Œw1_i & w2_jéƒ½å¯ä»¥è¢«è¨­ç‚º0ã€‚</br>
ä¸€æ¨£é€éregularization loss:</br> <a href="https://www.codecogs.com/eqnedit.php?latex=f(w)&space;=&space;\epsilon_1&space;\beta&space;w^2&space;/&space;(1&plus;\beta&space;w^2)&space;&plus;&space;\epsilon_2w^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?f(w)&space;=&space;\epsilon_1&space;\beta&space;w^2&space;/&space;(1&plus;\beta&space;w^2)&space;&plus;&space;\epsilon_2w^2" title="f(w) = \epsilon_1 \beta w^2 / (1+\beta w^2) + \epsilon_2w^2" /></a></br>
ä¾†é™åˆ¶weightã€‚</br>
ä¸»è¦é‚„æ˜¯åœ¨è¨è«–å¦‚ä½•åšnetwork pruningï¼Œé€™ç¯‡ä¸»è¦ç›¸é—œæ˜¯ä¸‹ä¸€ç¯‡åŒä½œè€…ä½¿ç”¨é€™ç¨®æ¼”ç®—æ³•ä¾†åšfeature selectionã€‚</br>
æœ‰æåˆ°ä¸€å€‹L2 lossçš„è®Šç¨®ï¼ŒLoss = w^2 / (1 + w^2)ï¼Œæœƒå°‡loss boundåœ¨[0, 1]ï¼Œé€™å¯ä»¥åŠ å…¥regularization loss è¨­è¨ˆè¨è«–ã€‚</br>


2. Neural-Network Feature Selector, IEEE, 1997. 427 citations.</br></br>
short critique : æ‰¿çºŒä¸Šç¯‡çš„lossï¼Œä½¿å¾—weightæœ‰sparsityçš„ç‰¹æ€§ï¼Œè¨è«–å¦‚ä½•åšfeature selectionã€‚</br>
Method : é¦–å…ˆä½¿ç”¨å…¨éƒ¨Nå€‹featureè¨“ç·´ä¸€å€‹networkï¼Œç„¶å¾Œå®šç¾©ä¸€å€‹èƒ½å¤ æ¥å—çš„æœ€å°performance dropï¼Œåœ¨é€™ç¯‡è£¡é¢ä½¿ç”¨accuracyã€‚</br>
å†ä¾†åˆ†åˆ¥æŠ½æ‰ä¸€å€‹featureï¼Œåˆ©ç”¨N-1å€‹featureè¨“ç·´å‡ºN-1å€‹networkï¼Œç„¶å¾Œå°performanceåšæ’åºï¼Œåªè¦æœ€å·®çš„é‚£å€‹networké‚„åœ¨å…è¨±performance dropç¯„åœå…§ï¼Œå°±ç¹¼çºŒæŠ½æ‰featureã€‚</br>
å…·é«”ä¾†èªªæ˜¯ä¸€å€‹greedy strategyï¼Œæœ‰é»åƒDecision Treeã€‚</br>
Drawbacks : é¦–å…ˆæ˜¯greedyçš„é †åºæœ‰æ²’æœ‰å½±éŸ¿ä¸ç¢ºå®šï¼Œé€™æ˜¯å°æ–¼exhausive search for optimal solutionçš„å–æ¨ã€‚å†è€…æ˜¯è¨“ç·´æ™‚é–“å¤ªéèª‡å¼µã€‚time complexity = O(N! - k!), k = final feature count.</br>

3. Feature screening using signal-to-noise ratios, nercom, 2000. 96 citations.</br></br>
short critique : é€™ç¯‡æ²’ä»€éº¼ä½œç”¨ï¼Œä¸»è¦æ˜¯æå‡ºçš„æ–¹æ³•ä¸¦ä¸å¯¦ç”¨ï¼Œä½†æ˜¯æä¾›äº†å¦ä¸€ç¨®è¡¡é‡feature saliency çš„æ–¹å¼ï¼ŒSNR basedã€‚</br></br>
support : è£¡é¢åˆ†äº†ä¸‰å€‹feature saliency measure. Partial derivative based, weighted based and SNR based.</br>
[å–®ç¨çš„critique][3]</br>

4. Feature selection with neural networks, 2001. 236 citaions.</br>
short critique : 
support : è£¡é¢æåˆ°äº†æœ‰weights-based(ä¸Šé¢é‚£ç¯‡)è·Ÿsignal-to-noise retioä»¥åŠoutput sensitivity basedä»¥åŠä¸‰å€‹æ–¹é¢ï¼Œsurveyçš„æ™‚å€™å¯ä»¥æ³¨æ„ã€‚</br>

## è¼ƒæ–°çš„works

5. Feature Selection Based on Structured Sparsity- A Comprehensive Study, IEEE, 2017. 95</br>
é€™æ˜¯ä¸€ç¯‡é‡å°æ¯”è¼ƒå¤è€æ–¹æ³•çš„reviewï¼Œæ²’æœ‰æä¾›å¤ªå¤šé¢å‘ï¼Œæ³¨é‡åœ¨ä»‹ç´¹Group Lasso, Graph Lassoï¼Œä»¥åŠä¸€äº›feature structureç­‰ç­‰ã€‚</br>
å¾ç°¡å–®çš„lasso linear regressioné–‹å§‹è«‡ï¼Œä»¥åŠä»–çš„è®Šå½¢group lasso, graph lassoç­‰ç­‰ã€‚</br>
å¦å¤–ä¸€å€‹æ¦‚å¿µæ˜¯multi-taskï¼Œç›´æ¥é€énorm of matrixä¾†é™åˆ¶Wï¼Œä¸€æ¨£åˆ©ç”¨norm p, 0 < p <= 1ï¼Œä¾†é€ æˆweight sparsityã€‚</br>
æ‡‰è©²æ˜¯æƒ³ä¿ç•™åœ¨multi-taskåº•ä¸‹ï¼Œèƒ½å¤ åšåˆ°feature selectionåœ¨feature spaceä¸ŠçœŸæ­£ç¨ç«‹ï¼Œè€Œä¸æ˜¯åªåœ¨ç‰¹å®šä»»å‹™ä¸‹ç¨ç«‹ï¼Œæœƒå¾—åˆ°æ¯”è¼ƒgeneralçš„feature setã€‚</br>
æ³¨æ„ä¸€ä¸‹matrix normã€‚ norm2,1æœƒé€ æˆå‡ºç¾zero rowè€Œnorm1,2 å‡ºç¾zero columnï¼Œ</br>
é€™ä»£è¡¨è¦å‡ºç¾dead neuronï¼Œå°æ‰€æœ‰connected linkçš„weightéƒ½æ˜¯é›¶ã€‚</br>
æˆ–æ˜¯filter neuronï¼Œåªå°ç‰¹å®šå¹¾å€‹connected linkçš„weightæ˜¯é›¶ã€‚</br>
é€™å°network pruningçš„è¨­è¨ˆæœ‰é»å¹«åŠ©ã€‚</br>

æˆ‘åœ¨é€™ç¯‡è£¡é¢å¾—åˆ°æ¯”è¼ƒæœ‰æ•ˆçš„è³‡è¨Šæ˜¯ï¼Œæœ‰é—œè§£norm p, 0 < p < 1çš„æ–¹å¼ï¼Œé€™å€‹éƒ¨åˆ†ä¹Ÿè¨±å¯ä»¥å†åŠ å…¥SelectNetï¼Œå½¢æˆæ›´åŠ sparseçš„weightã€‚</br>
åˆ†åˆ¥æ˜¯ä»¥ä¸‹[å…©ç¯‡](#norm-0.5)</br>
é‚„æœ‰ä¸€å€‹å¥½è™•æ˜¯æä¾›äº†ç›¸ç•¶å¤šæ–¹æ³•çš„å¯¦é©—çµæœï¼š</br>
å–®è®Šé‡çµ±è¨ˆçš„æœ‰Fisher Score, Gini, information Gain, T-test, Chi-Square test.</br>
é‚„æœ‰ä¸€äº›æˆ‘èªç‚ºæ¯”è¼ƒä¸å¸¸ç”¨çš„ï¼ŒL1 SVM, Lassoç­‰ç­‰ã€‚</br>
ä½†æ˜¯é€™äº›datasetéƒ½ä¸å¥½æ‰¾ï¼Œæœ‰é—œç”Ÿç‰©è³‡è¨Šçš„ï¼Œä¸æ˜¯éœ€è¦ç”³è«‹å°±æ˜¯è¦éŒ¢ï¼Œå¾Œé¢å¦‚æœæ‰¾ä¸åˆ°é©åˆçš„datasetå¯ä»¥å†å›ä¾†å˜—è©¦ã€‚</br>

6. Feature Selection- A Data Perspective, IEEE, 2018, 265</br>
é€™æ˜¯ä¸€ç¯‡éå¸¸å¥½çš„reviewï¼Œ[å–®ç¨critique][6]ï¼Œ



## norm 0.5
A unified algorithm for mixed   ğ‘™2,ğ‘ -minimizations and its application in feature selection, 2014. 23.</br>
Feature Selection at the Discrete Limit, AAAI, 2014. 31.</br>



æœ‰æä¾›ç›¸ç•¶å¤šçš„performanceæ¯”è¼ƒï¼Œæœ‰ç°¡å–®çš„å–®è®Šé‡åˆ†æ(chi-square, Fisher-score, etc.)</br>
ä»¥åŠæ–‡ä¸­æåˆ°çš„ä¸€äº›æ–¹æ³•ï¼Œä½†æ˜¯ä½¿ç”¨çš„datasetæœ‰äº›å•é¡Œï¼Œä¸æ˜¯å¤ªå¸¸è¦‹ï¼Œè€Œä¸”éœ€è¦ç”³è«‹ã€‚</br>
å†ä¾†æ˜¯feature selection algorithmï¼Œä¸¦æ²’æœ‰è©³ç´°èªªæ˜ï¼ŒçŒœæ¸¬æ˜¯å…ˆç”¨rankingï¼Œç„¶å¾Œé€ä¸€æ‹¿æ‰featureã€‚</br>

[3]: https://github.com/k123321141/SelectNet/blob/master/refs/Feature%20screening%20using%20signal-to-noise%20ratios%2C%20nercom%2C%202010%2C%2096/README.md
[8]: https://github.com/k123321141/SelectNet/blob/master/refs/Feature%20Selection-%20A%20Data%20Perspective%2C%20IEEE%2C%202018%2C%20265/README.md
[bmi_summary]: https://github.com/k123321141/SelectNet/blob/master/figures/bmi_summary.png
[bmi_w]: https://github.com/k123321141/SelectNet/blob/master/figures/bmi_w.png
[bmi_w_ratio]: https://github.com/k123321141/SelectNet/blob/master/figures/bmi_w_ratio.png
[mnist_overview_1]: https://github.com/k123321141/SelectNet/blob/master/figures/mnist_overview_1.png
[mnist_overview_2]: https://github.com/k123321141/SelectNet/blob/master/figures/mnist_overview_2.png
[mnist_ratio]: https://github.com/k123321141/SelectNet/blob/master/figures/mnist_overview.png
[mnist_mask_1]: https://github.com/k123321141/SelectNet/blob/master/figures/mnist_mask_1.png
[mnist_mask_2]: https://github.com/k123321141/SelectNet/blob/master/figures/mnist_mask_2.png
[cifar10_1]: https://github.com/k123321141/SelectNet/blob/master/figures/cifar10_1.png

