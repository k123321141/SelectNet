{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optimi\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.CustomDataset import CustomDataset\n",
    "from utils.SimpleDNN import SimpleDNN\n",
    "from SelectNet import *\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "def generator(dataloader):\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "\n",
    "def mask_column(X, col_idx):\n",
    "    assert len(X.shape) == 2\n",
    "    ret = X.clone()\n",
    "    ret[:, col_idx] = 0\n",
    "    return ret\n",
    "\n",
    "def add_masked_noise(x, std, col_idx):\n",
    "    normal = torch.distributions.Normal(0, std)\n",
    "    batch, dim = x.shape\n",
    "    ret_x = x.clone()\n",
    "    for idx in col_idx:\n",
    "        ret_x[:,idx] += normal.sample([batch,])\n",
    "    return ret_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synthesis BMI data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(arr, width=0.5):\n",
    "    plt.hist(arr, bins=np.arange(min(arr),max(arr), width))\n",
    "def F_bmi(w, h):\n",
    "    return w / np.power(h/100., 2)\n",
    "\n",
    "def F1(x):\n",
    "    return 1.3*np.power(x, 3) + 2*x - 17\n",
    "def F2(x, y):\n",
    "    return x*x + 3\n",
    "def F3(x, y):\n",
    "    return -0.2*x*y + 3*y + 15\n",
    "def F4(x, y):\n",
    "    return -2*x + y\n",
    "\n",
    "\n",
    "N = 5000\n",
    "\n",
    "\n",
    "iid = np.random.normal(77, 16, size=[N,1])\n",
    "iid2 = np.random.normal(22, 2, size=[N,1])\n",
    "h = np.random.normal(167, 5.7, size=[N,1]) \n",
    "w = np.random.normal(64.8, 8.9, size=[N,1])\n",
    "# h = np.random.uniform(60, 250, size=[N,1]) \n",
    "# w = np.random.uniform(30, 200, size=[N,1])\n",
    "\n",
    "h = np.clip(h, 40, 240)\n",
    "w = np.clip(w, 20, 200)\n",
    "bmi = F_bmi(w, h)\n",
    "\n",
    "x1 = np.random.uniform(-10, 10, size=[N, 1])\n",
    "y1 = F1(x1)\n",
    "\n",
    "x2 = np.random.normal(-30, 10, size=[N, 1])\n",
    "y2 = F2(x1, y1)\n",
    "y3 = F3(x1, x2)\n",
    "y4 = F4(y1, y2)\n",
    "\n",
    "\n",
    "# noise\n",
    "small_noise = np.random.normal(0, 1, size=[N,1])\n",
    "median_noise = np.random.normal(0, 10, size=[N,1])\n",
    "# median_noise = np.random.normal(0, 5, size=[N,1])\n",
    "bmi = bmi+small_noise\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "# ax = fig.gca()\n",
    "# ax.scatter(x1.flatten(), y4.flatten(), label='curve')\n",
    "# ax.scatter(x1.flatten(), y1.flatten(), label='curve')\n",
    "# ax.scatter(x1.flatten(), y1.flatten(), y2.flatten(), label='curve')\n",
    "ax.scatter(w.flatten()[:500], h.flatten()[:500], bmi.flatten()[:500], label='curve')\n",
    "# ax.scatter(w.flatten(), h.flatten(), label='curve')\n",
    "# ax.plot(x1.flatten(), y1.flatten(), y2.flatten(), label='curve')\n",
    "# ax.scatter(x1.flatten(), x2.flatten(), y2.flatten(), 'b.')\n",
    "# ax.scatter(h.flatten(), w.flatten(), bmi.flatten(), 'b.')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print w[0], h[0],bmi[0]\n",
    "plot_hist(bmi[np.where(bmi<=25)])\n",
    "plot_hist(bmi[np.where(bmi>25)])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X = torch.from_numpy(np.hstack([h, w, iid, iid2]).astype(np.float32))\n",
    "# X = torch.from_numpy(np.hstack([x1, x2, y1, y2, y3, iid, iid2]).astype(np.float32))\n",
    "# X = torch.from_numpy(np.hstack([x1, x2, y1, y2, y3, iid, iid2]).astype(np.float32))\n",
    "\n",
    "# X = torch.from_numpy(x1.astype(np.float32))\n",
    "# X = torch.from_numpy(np.hstack([x1, iid]).astype(np.float32))\n",
    "# X, norm = sklearn.preprocessing.normalize(X, axis=0, return_norm=True)\n",
    "# X = X.astype(np.float32)\n",
    "# print norm.shape\n",
    "Y = torch.from_numpy(bmi.astype(np.float32))\n",
    "# Y = torch.from_numpy(y4.astype(np.float32))\n",
    "# Y = torch.from_numpy((bmi > 25).astype(np.float32))\n",
    "print X.shape, Y.shape\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(CustomDataset(X, Y), [N-N//10, N//10])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_G = generator(train_dataloader)\n",
    "val_G = generator(val_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "DNN_model = SimpleDNN(X.shape[-1], 1024, 1, 3, F.relu)\n",
    "model = SelectNet(X.shape[-1], DNN_model, DNN_model.kernel_weights).cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "alpha = 0.1\n",
    "beta = 0\n",
    "gamma = 0\n",
    "epochs = 1000\n",
    "iters = 1\n",
    "noise_std = 25\n",
    "noise_col_idx = [2, 3]\n",
    "writer = SummaryWriter('./AE_logs/noised_bmi-a%f,b%f,g%f' % (alpha, beta, gamma))\n",
    "with tqdm(total=epochs*len(train_dataloader)) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "#         mask_idx = np.random.randint(0, X.shape[-1])\n",
    "        for _ in range(len(train_dataloader)):\n",
    "            x, y = next(train_G)\n",
    "            noised_x = add_masked_noise(x, noise_std, noise_col_idx)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            noised_x = noised_x.cuda()\n",
    "            val_x, val_y = next(val_G)\n",
    "            val_noised_x = add_masked_noise(val_x, noise_std, noise_col_idx)\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "            val_noised_x = val_noised_x.cuda()\n",
    "#             mask\n",
    "#             x = mask_column(x, mask_idx)\n",
    "#             val_x = mask_column(val_x, mask_idx)\n",
    "# \n",
    "            model.train()\n",
    "            out = model(x)\n",
    "            reg_loss, w_loss, entropy_loss = model.calc_reg_loss(F.mse_loss)\n",
    "            mse_loss = F.mse_loss(out, y)\n",
    "            loss = alpha*reg_loss + beta*w_loss + gamma*entropy_loss + mse_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "                out = model(val_x)\n",
    "                val_mse_loss = F.mse_loss(out, val_y)\n",
    "#                 noised\n",
    "                noised_mse = F.mse_loss(model(noised_x), y)\n",
    "                val_noised_mse = F.mse_loss(model(val_noised_x), val_y)\n",
    "                \n",
    "            \n",
    "            pbar.update(1)\n",
    "            w_arr = model.w.cpu().detach().numpy().flatten()\n",
    "            w_prine = torch.sigmoid(model.w).cpu().detach().numpy().flatten()\n",
    "            w_ratio = model.select_lay.calc_ratio().cpu().detach().numpy().flatten()\n",
    "#             buf = ','.join(['%d:%.2f' % (i+1,x) for i,x in enumerate(buf)])\n",
    "            buf = ','.join(['%2.3f, ' % (x) for i,x in enumerate(w_ratio)])\n",
    "            pbar.set_postfix_str('loss: %.3f, val_loss: %.4f, w_loss : %.3f, entropy : %.3f, regularizer : %.3f                     %s' %\n",
    "                                 (mse_loss.item(), val_mse_loss.item(), \n",
    "                                  w_loss.item(), entropy_loss.item(),\n",
    "                                  reg_loss.item(), buf))\n",
    "#             if mse_loss.item() < 100:\n",
    "            if epoch > 0:\n",
    "                writer.add_scalars('data/loss', {'train': loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/mse_loss', {'train': mse_loss.item(),\n",
    "                                                     'validation': val_mse_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/noised_mse_loss', {'train': noised_mse.item(),\n",
    "                                                     'validation': val_noised_mse.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_loss', {'train': w_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/entropy', {'train': entropy_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/reg_loss', {'train': reg_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w', {'w%d' % (i+1) : v  for i, v in enumerate(w_arr)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w\\'', {'w%d' % (i+1) : v  for i, v in enumerate(w_prine)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_ratio', {'w%d' % (i+1) : v  for i, v in enumerate(w_ratio)},\n",
    "                                                     iters)\n",
    "            \n",
    "            iters += 1\n",
    "\n",
    "print 'done 1'\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print 'done'\n",
    "\n",
    "print att_w[0,: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170000/170000 [32:15<00:00, 87.83it/s, acc : 1.000, val_acc : 1.000, loss: 0.009, val_loss: 0.0112, w_loss : 0.000, entropy : 1.572, regularizer : 0.451                     0.235, ,0.012, ,0.495, ,0.258, ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 1\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import sklearn.metrics\n",
    "\n",
    "def calc_accracy(y, out):\n",
    "    label = y.flatten().cpu().detach().numpy().astype(np.int)\n",
    "    pred = torch.argmax(out, dim=-1).cpu().detach().numpy().astype(np.int)\n",
    "    return sklearn.metrics.accuracy_score(label, pred)\n",
    "            \n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "Y = iris.target.reshape([-1, 1]).astype(np.float32)\n",
    "# print iris.data.shape\n",
    "N = len(Y)\n",
    "batch_size = 8\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(CustomDataset(X, Y), [N-N//10, N//10])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_G = generator(train_dataloader)\n",
    "val_G = generator(val_dataloader)\n",
    "\n",
    "\n",
    "DNN_model = SimpleDNN(X.shape[-1], 16, 3, 2, F.relu)\n",
    "model = SelectNet(X.shape[-1], DNN_model, DNN_model.kernel_weights).cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "alpha = 0.1\n",
    "beta = 100\n",
    "gamma = 0\n",
    "epochs = 10000\n",
    "iters = 1\n",
    "noise_std = 25\n",
    "noise_col_idx = []\n",
    "writer = SummaryWriter('./AE_logs/Iris-a%f,b%f,g%f' % (alpha, beta, gamma))\n",
    "src_loss_criterion = nn.CrossEntropyLoss()\n",
    "with tqdm(total=epochs*len(train_dataloader)) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "#         mask_idx = np.random.randint(0, X.shape[-1])\n",
    "        for _ in range(len(train_dataloader)):\n",
    "            x, y = next(train_G)\n",
    "            noised_x = add_masked_noise(x, noise_std, noise_col_idx)\n",
    "            x, y = x.cuda(), y.type(torch.long).flatten().cuda()\n",
    "            noised_x = noised_x.cuda()\n",
    "            val_x, val_y = next(val_G)\n",
    "            val_noised_x = add_masked_noise(val_x, noise_std, noise_col_idx)\n",
    "            val_x, val_y = val_x.cuda(), val_y.type(torch.long).flatten().cuda()\n",
    "            val_noised_x = val_noised_x.cuda()\n",
    "#             mask\n",
    "#             x = mask_column(x, mask_idx)\n",
    "#             val_x = mask_column(val_x, mask_idx)\n",
    "# \n",
    "            model.train()\n",
    "#             train_out = torch.softmax(model(x), dim=-1)\n",
    "            train_out = model(x)\n",
    "            reg_loss, w_loss, entropy_loss = model.calc_reg_loss(F.mse_loss)\n",
    "            src_loss = src_loss_criterion(train_out, y)\n",
    "            loss = alpha*reg_loss + beta*w_loss + gamma*entropy_loss + src_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "#                 val_out = torch.softmax(model(val_x), dim=-1)\n",
    "                val_out = model(val_x)\n",
    "                val_src_loss = src_loss_criterion(val_out, val_y)\n",
    "                \n",
    "                \n",
    "#                 noised\n",
    "                noised_train_out = model(noised_x)\n",
    "                noised_val_out = model(val_noised_x)\n",
    "                noised_src = src_loss_criterion(noised_train_out, y)\n",
    "                val_noised_src = src_loss_criterion(noised_val_out, val_y)\n",
    "#                 acc\n",
    "                train_acc = calc_accracy(y, train_out)\n",
    "                val_acc = calc_accracy(val_y, val_out)\n",
    "                noised_train_acc = calc_accracy(y, noised_train_out)\n",
    "                noised_val_acc = calc_accracy(val_y, noised_val_out)\n",
    "                \n",
    "            \n",
    "            \n",
    "            pbar.update(1)\n",
    "            w_arr = model.w.cpu().detach().numpy().flatten()\n",
    "            w_prine = torch.sigmoid(model.w).cpu().detach().numpy().flatten()\n",
    "            w_ratio = model.select_lay.calc_ratio().cpu().detach().numpy().flatten()\n",
    "#             buf = ','.join(['%d:%.2f' % (i+1,x) for i,x in enumerate(buf)])\n",
    "            buf = ','.join(['%2.3f, ' % (x) for i,x in enumerate(w_ratio)])\n",
    "            pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss: %.3f, val_loss: %.4f, w_loss : %.3f, entropy : %.3f, regularizer : %.3f                     %s' %\n",
    "                                 (\n",
    "                                     train_acc.item(), val_acc.item(),\n",
    "                                     src_loss.item(), val_src_loss.item(), \n",
    "                                     w_loss.item(), entropy_loss.item(),\n",
    "                                     reg_loss.item(), buf))\n",
    "            if epoch > 0:\n",
    "                writer.add_scalars('data/loss', {'train': loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/cross-entropy', {'train': src_loss.item(),\n",
    "                                                     'validation': val_src_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/noised_loss', {'train': noised_src.item(),\n",
    "                                                     'validation': val_noised_src.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/accuracy', {'train': train_acc.item(),\n",
    "                                                     'validation': val_acc.item(),\n",
    "                                                     'noised_train': noised_train_acc.item(),\n",
    "                                                     'noised_validation': noised_val_acc.item(),\n",
    "                                                    },\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_loss', {'train': w_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/entropy', {'train': entropy_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/reg_loss', {'train': reg_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w', {'w%d' % (i+1) : v  for i, v in enumerate(w_arr)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_prine', {'w%d' % (i+1) : v  for i, v in enumerate(w_prine)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_ratio', {'w%d' % (i+1) : v  for i, v in enumerate(w_ratio)},\n",
    "                                                     iters)\n",
    "            \n",
    "            iters += 1\n",
    "\n",
    "print 'done 1'\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print 'done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3112/590000 [37:16<118:43:33,  1.37it/s, acc : 0.762, val_acc : 0.788, loss: 0.761, val_loss: 0.6737, w_loss : 0.135, entropy : 9.615, regularizer : 0.539 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-796c3ca2d316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m                                                      iters)\n\u001b[1;32m    125\u001b[0m                 writer.add_scalars('data/w', {'w%d' % (i+1) : v  for i, v in enumerate(w_arr)},\n\u001b[0;32m--> 126\u001b[0;31m                                                      iters)\n\u001b[0m\u001b[1;32m    127\u001b[0m                 writer.add_scalars('data/w_prine', {'w%d' % (i+1) : v  for i, v in enumerate(w_prine)},\n\u001b[1;32m    128\u001b[0m                                                      iters)\n",
      "\u001b[0;32m/home/k123/miniconda2/envs/python-conda/lib/python2.7/site-packages/tensorboardX/writer.pyc\u001b[0m in \u001b[0;36madd_scalars\u001b[0;34m(self, main_tag, tag_scalar_dict, global_step, walltime)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_scalar_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mfw_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfw_logdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmain_tag\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mfw_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfw_tag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import sklearn.metrics\n",
    "\n",
    "def calc_accracy(y, out):\n",
    "    label = y.flatten().cpu().detach().numpy().astype(np.int)\n",
    "    pred = torch.argmax(out, dim=-1).cpu().detach().numpy().astype(np.int)\n",
    "    return sklearn.metrics.accuracy_score(label, pred)\n",
    "            \n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                           transforms.Lambda(lambda x:x.flatten()),\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                           transforms.Lambda(lambda x:x.flatten()),\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_G = generator(train_dataloader)\n",
    "val_G = generator(val_dataloader)\n",
    "\n",
    "X = np.zeros([50000, 784])\n",
    "\n",
    "DNN_model = SimpleDNN(X.shape[-1], 16, 10, 3, F.relu)\n",
    "model = SelectNet(X.shape[-1], DNN_model, DNN_model.kernel_weights).cuda()\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "alpha = 0.1\n",
    "beta = 100\n",
    "gamma = 0\n",
    "epochs = 10000\n",
    "iters = 1\n",
    "noise_std = 25\n",
    "noise_col_idx = []\n",
    "writer = SummaryWriter('./AE_logs/mnist-a%f,b%f,g%f' % (alpha, beta, gamma))\n",
    "src_loss_criterion = nn.CrossEntropyLoss()\n",
    "with tqdm(total=epochs*len(train_dataloader)) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "#         mask_idx = np.random.randint(0, X.shape[-1])\n",
    "        for _ in range(len(train_dataloader)):\n",
    "            x, y = next(train_G)\n",
    "            noised_x = add_masked_noise(x, noise_std, noise_col_idx)\n",
    "            x, y = x.cuda(), y.type(torch.long).flatten().cuda()\n",
    "            noised_x = noised_x.cuda()\n",
    "            val_x, val_y = next(val_G)\n",
    "            val_noised_x = add_masked_noise(val_x, noise_std, noise_col_idx)\n",
    "            val_x, val_y = val_x.cuda(), val_y.type(torch.long).flatten().cuda()\n",
    "            val_noised_x = val_noised_x.cuda()\n",
    "#             mask\n",
    "#             x = mask_column(x, mask_idx)\n",
    "#             val_x = mask_column(val_x, mask_idx)\n",
    "# \n",
    "            model.train()\n",
    "#             train_out = torch.softmax(model(x), dim=-1)\n",
    "            train_out = model(x)\n",
    "            reg_loss, w_loss, entropy_loss = model.calc_reg_loss(F.mse_loss)\n",
    "            src_loss = src_loss_criterion(train_out, y)\n",
    "            loss = alpha*reg_loss + beta*w_loss + gamma*entropy_loss + src_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "            \n",
    "#                 val_out = torch.softmax(model(val_x), dim=-1)\n",
    "                val_out = model(val_x)\n",
    "                val_src_loss = src_loss_criterion(val_out, val_y)\n",
    "                \n",
    "                \n",
    "#                 noised\n",
    "                noised_train_out = model(noised_x)\n",
    "                noised_val_out = model(val_noised_x)\n",
    "                noised_src = src_loss_criterion(noised_train_out, y)\n",
    "                val_noised_src = src_loss_criterion(noised_val_out, val_y)\n",
    "#                 acc\n",
    "                train_acc = calc_accracy(y, train_out)\n",
    "                val_acc = calc_accracy(val_y, val_out)\n",
    "                noised_train_acc = calc_accracy(y, noised_train_out)\n",
    "                noised_val_acc = calc_accracy(val_y, noised_val_out)\n",
    "                \n",
    "            \n",
    "            \n",
    "            pbar.update(1)\n",
    "            w_arr = model.w.cpu().detach().numpy().flatten()\n",
    "            w_prine = torch.sigmoid(model.w).cpu().detach().numpy().flatten()\n",
    "            w_ratio = model.select_lay.calc_ratio().cpu().detach().numpy().flatten()\n",
    "#             buf = ','.join(['%d:%.2f' % (i+1,x) for i,x in enumerate(buf)])\n",
    "            pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss: %.3f, val_loss: %.4f, w_loss : %.3f, entropy : %.3f, regularizer : %.3f ' %\n",
    "                                 (\n",
    "                                     train_acc.item(), val_acc.item(),\n",
    "                                     src_loss.item(), val_src_loss.item(), \n",
    "                                     w_loss.item(), entropy_loss.item(),\n",
    "                                     reg_loss.item()))\n",
    "            if epoch > 0:\n",
    "                writer.add_scalars('data/loss', {'train': loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/cross-entropy', {'train': src_loss.item(),\n",
    "                                                     'validation': val_src_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/noised_loss', {'train': noised_src.item(),\n",
    "                                                     'validation': val_noised_src.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/accuracy', {'train': train_acc.item(),\n",
    "                                                     'validation': val_acc.item(),\n",
    "                                                     'noised_train': noised_train_acc.item(),\n",
    "                                                     'noised_validation': noised_val_acc.item(),\n",
    "                                                    },\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_loss', {'train': w_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/entropy', {'train': entropy_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/reg_loss', {'train': reg_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w', {'w%d' % (i+1) : v  for i, v in enumerate(w_arr)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_prine', {'w%d' % (i+1) : v  for i, v in enumerate(w_prine)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w_ratio', {'w%d' % (i+1) : v  for i, v in enumerate(w_ratio)},\n",
    "                                                     iters)\n",
    "            \n",
    "            iters += 1\n",
    "\n",
    "print 'done 1'\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print 'done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 784]) torch.Size([1024]) 59\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import sklearn.metrics\n",
    "\n",
    "def calc_accracy(y, out):\n",
    "    label = y.flatten().cpu().detach().numpy().astype(np.int)\n",
    "    pred = torch.argmax(out, dim=-1).cpu().detach().numpy().astype(np.int)\n",
    "    return sklearn.metrics.accuracy_score(label, pred)\n",
    "            \n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def eval_noise(model, val_G, label, iters=100):\n",
    "    x_ax = np.linspace(1, 30, 100)\n",
    "    src_arr = []\n",
    "    noised_arr = []\n",
    "    for std in x_ax:\n",
    "        src_mse_loss = 0\n",
    "        noised_mse_loss = 0\n",
    "        normal = torch.distributions.Normal(0, std)\n",
    "        for i in range(iters):\n",
    "            val_x, val_y = next(val_G)\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "            N = val_x.shape[0]\n",
    "            noise = torch.cat([torch.zeros_like(val_x)[:,:-2], normal.sample([N,2]).cuda()], dim=1)\n",
    "            out1,_ = model(val_x)\n",
    "            out2,_ = model(val_x + noise)\n",
    "            src_mse_loss += F.mse_loss(out1, val_y)\n",
    "            noised_mse_loss += F.mse_loss(out2, val_y)\n",
    "        src_mse_loss /= iters\n",
    "        noised_mse_loss /= iters\n",
    "        src_arr.append(src_mse_loss.item())\n",
    "        noised_arr.append(noised_mse_loss.item())\n",
    "    \n",
    "#     plt.plot(x_ax, src_arr, label=label + ' mse')\n",
    "    plt.plot(x_ax, noised_arr, label=label + ' mse with noise')\n",
    "# eval_noise(att_model, val_G, 'selection', 10)\n",
    "eval_noise(model, val_G, 'src', 10 )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "print X.shape, Y.shape\n",
    "print Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2353/15000000 [00:28<51:17:24, 81.22it/s, loss: 0.036, val_loss: 0.0209, att_loss : 21.284, regularizer : 0.008                     0.253, ,0.249, ,0.249, ,0.249, l1 0.213, entropy 2.000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-915c8a1df8fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_G\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data.astype(np.float32)\n",
    "Y = iris.target.reshape([-1, 1]).astype(np.float32)\n",
    "# print iris.data.shape\n",
    "N = len(Y)\n",
    "batch_size = 32\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(CustomDataset(X, Y), [N-N//10, N//10])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_G = generator(train_dataloader)\n",
    "val_G = generator(val_dataloader)\n",
    "\n",
    "model = Attention_Autoendecoder(X.shape[-1]).cuda()\n",
    "# opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "opt = optim.Adam(model.parameters())\n",
    "alpha = 100\n",
    "beta = 0\n",
    "epochs = 1000\n",
    "iters = 1\n",
    "noise_std = 5\n",
    "noise_col_idx = [2, 3]\n",
    "writer = SummaryWriter('./AE_logs/Iris-a%f,b%f' % (alpha, beta))\n",
    "reg_l2_coe = 0.1\n",
    "with tqdm(total=epochs*len(train_dataloader)) as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        mask_idx = np.random.randint(0, X.shape[-1])\n",
    "        for _ in range(len(train_dataloader)):\n",
    "            x, y = next(train_G)\n",
    "            noised_x = add_masked_noise(x, noise_std, noise_col_idx)\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            noised_x = noised_x.cuda()\n",
    "            val_x, val_y = next(val_G)\n",
    "            val_noised_x = add_masked_noise(val_x, noise_std, noise_col_idx)\n",
    "            val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "            val_noised_x = val_noised_x.cuda()\n",
    "#             mask\n",
    "#             x = mask_column(x, mask_idx)\n",
    "#             val_x = mask_column(val_x, mask_idx)\n",
    "# \n",
    "            model.train()\n",
    "            out, att_w = model(x)\n",
    "            w_ = torch.softmax(att_w, dim=-1)\n",
    "            mse_loss = F.mse_loss(out, y)\n",
    "            att_loss = alpha*(F.l1_loss(att_w, torch.zeros_like(att_w)) + beta*entropy_loss(w_))\n",
    "            reg_loss = reg_l2_coe * regularizer(model.kernel_weights, F.mse_loss)\n",
    "            loss = att_loss + mse_loss + reg_loss\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                out, _ = model(val_x)\n",
    "                val_mse_loss = F.mse_loss(out, val_y)\n",
    "                val_loss = att_loss + val_mse_loss + reg_loss\n",
    "#                 noised\n",
    "                noised_mse = F.mse_loss(model(noised_x)[0], y)\n",
    "                val_noised_mse = F.mse_loss(model(val_noised_x)[0], val_y)\n",
    "                \n",
    "            \n",
    "            pbar.update(1)\n",
    "            w_arr = model.w.cpu().detach().numpy().flatten()\n",
    "            att_w_arr = att_w.cpu().detach().numpy().flatten()\n",
    "            ratio_w = (att_w / torch.sum(att_w)).cpu().detach().numpy().flatten()\n",
    "#             buf = ','.join(['%d:%.2f' % (i+1,x) for i,x in enumerate(buf)])\n",
    "            buf = ','.join(['%2.3f, ' % (x) for i,x in enumerate(ratio_w)])\n",
    "            buf += 'l1 %.3f, entropy %.3f' % (F.l1_loss(att_w, torch.zeros_like(att_w)).item(), entropy_loss(w_).item())\n",
    "            pbar.set_postfix_str('loss: %.3f, val_loss: %.4f, att_loss : %.3f, regularizer : %.3f                     %s' %\n",
    "                                 (mse_loss.item(), val_mse_loss.item(), \n",
    "                                  att_loss.item(),\n",
    "                                  reg_loss.item(), buf))\n",
    "#             if mse_loss.item() < 100:\n",
    "            if epoch > 0:\n",
    "                writer.add_scalars('data/loss', {'train': loss.item(),\n",
    "                                                     'validation': val_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/mse_loss', {'train': mse_loss.item(),\n",
    "                                                     'validation': val_mse_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/noised_mse_loss', {'train': noised_mse.item(),\n",
    "                                                     'validation': val_noised_mse.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/att_loss', {'train': att_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/reg_loss', {'train': reg_loss.item()},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/w', {'w%d' % (i+1) : v  for i, v in enumerate(w_arr)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/att_w', {'w%d' % (i+1) : v  for i, v in enumerate(att_w_arr)},\n",
    "                                                     iters)\n",
    "                writer.add_scalars('data/ratio_att_w', {'w%d' % (i+1) : v  for i, v in enumerate(ratio_w)},\n",
    "                                                     iters)\n",
    "            \n",
    "            iters += 1\n",
    "\n",
    "print 'done 1'\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print 'done'\n",
    "\n",
    "print att_w[0,: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2940, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "src_mse_loss = 0\n",
    "noised_mse_loss = 0\n",
    "normal = torch.distributions.Normal(0, 20)\n",
    "\n",
    "for i in range(100):\n",
    "    val_x, val_y = next(val_G)\n",
    "    val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "    N = val_x.shape[0]\n",
    "    noise = torch.cat([torch.zeros_like(val_x)[:,:-2], normal.sample([N,2]).cuda()], dim=1)\n",
    "    out1,_ = model(val_x)\n",
    "    out2,_ = model(val_x + noise)\n",
    "    src_mse_loss += F.mse_loss(out1, val_y)\n",
    "    noised_mse_loss += F.mse_loss(out2, val_y)\n",
    "    \n",
    "print src_mse_loss\n",
    "print noised_mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(torch.FloatTensor([1,0,0]), dim=0)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 4)\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 0s 94us/step - loss: 164.5978 - val_loss: 35.5823\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 26.3120 - val_loss: 21.2340\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 18.7055 - val_loss: 16.4996\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 13.3884 - val_loss: 11.8063\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 9.3696 - val_loss: 8.5097\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 6.4847 - val_loss: 5.7052\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 4.5516 - val_loss: 4.2466\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 3.6111 - val_loss: 3.6114\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 3.2435 - val_loss: 3.4421\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 3.0780 - val_loss: 3.1609\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.9451 - val_loss: 3.0375\n",
      "Epoch 12/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.8663 - val_loss: 3.0016\n",
      "Epoch 13/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.7417 - val_loss: 2.8099\n",
      "Epoch 14/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.6861 - val_loss: 2.7259\n",
      "Epoch 15/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.6115 - val_loss: 2.7234\n",
      "Epoch 16/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.5378 - val_loss: 2.8168\n",
      "Epoch 17/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.5584 - val_loss: 2.5762\n",
      "Epoch 18/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.3968 - val_loss: 2.5755\n",
      "Epoch 19/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.3645 - val_loss: 2.5532\n",
      "Epoch 20/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 2.3141 - val_loss: 2.3344\n",
      "Epoch 21/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.2628 - val_loss: 2.2973\n",
      "Epoch 22/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.2361 - val_loss: 2.2308\n",
      "Epoch 23/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.1911 - val_loss: 2.2188\n",
      "Epoch 24/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.1626 - val_loss: 2.1583\n",
      "Epoch 25/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.1601 - val_loss: 2.4074\n",
      "Epoch 26/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.0932 - val_loss: 2.2406\n",
      "Epoch 27/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.1047 - val_loss: 2.0838\n",
      "Epoch 28/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.0293 - val_loss: 2.0568\n",
      "Epoch 29/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.0044 - val_loss: 2.0361\n",
      "Epoch 30/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.0142 - val_loss: 2.1137\n",
      "Epoch 31/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.0231 - val_loss: 2.0176\n",
      "Epoch 32/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 2.0127 - val_loss: 2.0045\n",
      "Epoch 33/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.9483 - val_loss: 1.9880\n",
      "Epoch 34/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.9451 - val_loss: 1.9417\n",
      "Epoch 35/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.9728 - val_loss: 1.9857\n",
      "Epoch 36/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.9086 - val_loss: 2.2235\n",
      "Epoch 37/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.9335 - val_loss: 1.9293\n",
      "Epoch 38/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.9159 - val_loss: 1.9108\n",
      "Epoch 39/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.8834 - val_loss: 1.8783\n",
      "Epoch 40/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.8750 - val_loss: 1.8892\n",
      "Epoch 41/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.8784 - val_loss: 1.8679\n",
      "Epoch 42/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.8919 - val_loss: 1.9105\n",
      "Epoch 43/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.8299 - val_loss: 1.8592\n",
      "Epoch 44/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.8772 - val_loss: 1.8034\n",
      "Epoch 45/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.8548 - val_loss: 1.8141\n",
      "Epoch 46/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.7914 - val_loss: 1.7758\n",
      "Epoch 47/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.7801 - val_loss: 1.7811\n",
      "Epoch 48/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.7891 - val_loss: 1.8343\n",
      "Epoch 49/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.7516 - val_loss: 1.8223\n",
      "Epoch 50/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.7779 - val_loss: 1.7346\n",
      "Epoch 51/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.7482 - val_loss: 1.7103\n",
      "Epoch 52/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.6952 - val_loss: 2.1575\n",
      "Epoch 53/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.7600 - val_loss: 1.7288\n",
      "Epoch 54/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.7127 - val_loss: 2.0426\n",
      "Epoch 55/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.7550 - val_loss: 1.6996\n",
      "Epoch 56/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.6884 - val_loss: 1.6397\n",
      "Epoch 57/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.6621 - val_loss: 1.6292\n",
      "Epoch 58/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.6228 - val_loss: 1.9521\n",
      "Epoch 59/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.6458 - val_loss: 1.6139\n",
      "Epoch 60/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.5981 - val_loss: 1.6028\n",
      "Epoch 61/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.6479 - val_loss: 2.2519\n",
      "Epoch 62/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.7385 - val_loss: 1.5491\n",
      "Epoch 63/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.5461 - val_loss: 1.6900\n",
      "Epoch 64/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.5418 - val_loss: 1.5141\n",
      "Epoch 65/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.6640 - val_loss: 2.0320\n",
      "Epoch 66/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.5763 - val_loss: 1.5758\n",
      "Epoch 67/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.5135 - val_loss: 1.4786\n",
      "Epoch 68/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.4988 - val_loss: 1.5814\n",
      "Epoch 69/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.4454 - val_loss: 1.7496\n",
      "Epoch 70/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.4864 - val_loss: 1.6778\n",
      "Epoch 71/100\n",
      "4500/4500 [==============================] - 0s 12us/step - loss: 1.4527 - val_loss: 1.4736\n",
      "Epoch 72/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.5079 - val_loss: 1.5556\n",
      "Epoch 73/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.4562 - val_loss: 1.4216\n",
      "Epoch 74/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.4826 - val_loss: 1.7296\n",
      "Epoch 75/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.4799 - val_loss: 1.3373\n",
      "Epoch 76/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.3461 - val_loss: 1.9145\n",
      "Epoch 77/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.4652 - val_loss: 1.5242\n",
      "Epoch 78/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.3841 - val_loss: 1.6351\n",
      "Epoch 79/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.3999 - val_loss: 1.8342\n",
      "Epoch 80/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.3980 - val_loss: 1.2487\n",
      "Epoch 81/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.3465 - val_loss: 1.2815\n",
      "Epoch 82/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.3276 - val_loss: 1.3320\n",
      "Epoch 83/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.2186 - val_loss: 1.2296\n",
      "Epoch 84/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.2318 - val_loss: 1.1878\n",
      "Epoch 85/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.1846 - val_loss: 1.2444\n",
      "Epoch 86/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.2059 - val_loss: 1.1631\n",
      "Epoch 87/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.1552 - val_loss: 1.1328\n",
      "Epoch 88/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.1419 - val_loss: 1.1649\n",
      "Epoch 89/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.1701 - val_loss: 1.1866\n",
      "Epoch 90/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.1304 - val_loss: 1.0847\n",
      "Epoch 91/100\n",
      "4500/4500 [==============================] - 0s 11us/step - loss: 1.0982 - val_loss: 1.1542\n",
      "Epoch 92/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.0769 - val_loss: 1.1025\n",
      "Epoch 93/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.0389 - val_loss: 1.0256\n",
      "Epoch 94/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.0506 - val_loss: 1.0156\n",
      "Epoch 95/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.0055 - val_loss: 1.6745\n",
      "Epoch 96/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 1.2171 - val_loss: 1.1321\n",
      "Epoch 97/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 0.9795 - val_loss: 1.0032\n",
      "Epoch 98/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 0.9938 - val_loss: 0.9459\n",
      "Epoch 99/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 0.9795 - val_loss: 0.9928\n",
      "Epoch 100/100\n",
      "4500/4500 [==============================] - 0s 10us/step - loss: 0.9594 - val_loss: 1.0090\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "input_x = np.hstack([h, w, iid, iid2])\n",
    "print input_x.shape\n",
    "m = Sequential()\n",
    "m.add(Dense(32, activation='selu'))\n",
    "m.add(Dense(32, activation='selu'))\n",
    "m.add(Dense(1, activation='linear'))\n",
    "m.compile(loss='mse', optimizer='adam')\n",
    "m.fit(input_x, bmi, epochs=100, batch_size=128, validation_split=0.1)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print x1.shape\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(x1.flatten(), y1.flatten(), y2.flatten(), label='curve')\n",
    "ax.legend()\n",
    "print X.shape\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)\n",
    "z = np.linspace(-2, 2, 100)\n",
    "r = z**2 + 1\n",
    "x = r * np.sin(theta)\n",
    "y = r * np.cos(theta)\n",
    "ax.plot(x, y, z, label='parametric curve')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.14 (conda)",
   "language": "python",
   "name": "python-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
